---
title: "ML applications in breast cancer"
output: html_notebook
---

## Context

Breast cancer is used here an example application of machine learning due to several key factors:

- Large Datasets: There are extensive datasets available for breast cancer research, including gene expression data, clinical information, and patient outcomes. These large datasets provide ample data for training and evaluating machine learning models.

- Complex Relationships: Breast cancer is a complex disease with multiple factors influencing its development and progression. Machine learning algorithms can effectively identify intricate patterns and relationships within the data that might not be apparent to human analysis.

- Personalized Treatment: Machine learning can assist in developing personalized treatment plans by identifying patient subgroups with similar characteristics and predicting their response to different therapies.

The first example will consider the *ER status* of a tumour. ER Status (Estrogen Receptor Status) is a crucial factor in breast cancer. It refers to the presence or absence of estrogen receptors on breast cancer cells.

- ER-Positive: If breast cancer cells have estrogen receptors, they are considered ER-positive. These cells are sensitive to estrogen, and the hormone can stimulate their growth.
- ER-Negative: If breast cancer cells do not have estrogen receptors, they are considered ER-negative. These cells are less likely to be influenced by estrogen.

ER status is traditionally measured using immunohistochemistry (IHC). In IHC, a tissue sample from the breast tumor is treated with antibodies that specifically bind to estrogen receptors. If the antibodies bind to the tumor cells, it indicates the presence of estrogen receptors, making the tumor ER-positive.

**Can we use Machine learning to classify the ER status of a tumour based on omics data?** For this we will use a version of the TCGA dataset that has been processed to include various RNA-seq measurements from a set of genes, along with clinical characteristics of patients.


```{r}
data <- readRDS("brca_ML_example.rds")
data 
```


## Logistic Regression

First we prepare the data ready for analysis. To make the classification easier we will just use ER positive or negative samples (some of samples are undefined).

```{r}
library(tidymodels)

data_er <- filter(data, er_status_by_ihc %in% c("Positive","Negative")) %>% 
  dplyr::rename(ER_Status = er_status_by_ihc) %>% 
  mutate(ER_Status = as.factor(ER_Status))
```

We always split the data into training and test sets; with more samples allocated to training.

```{r}
set.seed(123)  # Set a seed for reproducibility
split <- initial_split(data_er, prop = 0.8,strata = ER_Status)  # Split data into 80% training and 20% testing

train_data <- training(split)
test_data <- testing(split)
```

Define the model and fit

```{r}
# Specify the model
glm_spec <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

# Fit the model
glm_fit <- fit(glm_spec, ER_Status ~ ESR1, data = train_data)

glm_fit %>% tidy

```

We want to use the model to make predictions using the test data - which importantly was not used during the training step. Since we know the true labels we can test how effective our model is. A simple check is to compare the predicted classes side-by-side to the actual classes.

```{r}
# Predict classes on the test set
predictions <- predict(glm_fit, new_data = test_data, type = "class")

# Create a data frame with actual and predicted values
results <- bind_cols(test_data, predictions)
## how many were correct?
dplyr::count(results, ER_Status, .pred_class)
```

We can more formally assess the model using a series of metrics provided by the `yardstick` package. We'll save these results to a data frame so we can compare between different models.

```{r}
# Calculate metrics
Metric_Summary <- data.frame(Accuracy = accuracy(results, truth = ER_Status, estimate = .pred_class)$.estimate, 
                  Precision=precision(results, truth = ER_Status, estimate = .pred_class)$.estimate,
                  Recall = recall(results, truth = ER_Status, estimate = .pred_class)$.estimate,
                  F1 = f_meas(results, truth = ER_Status, estimate = .pred_class)$.estimate,
                  Method = "Logistic Regression")
Metric_Summary
```
Some insight into the model can be gained by plotting the classifications against the values of ESR1. At a certain cut-off on ESR1, the model decides to call a tumour as ER positive.

```{r}
# Visualize predicted classes
ggplot(results, aes(x = ESR1, y = .pred_class,col=ER_Status)) +
  geom_jitter(height=0.1) +
  labs(x = "ESR1 Expression", y = "Predicted Probability of ER-Positive") 
```
This can also be visualised if we choose to get classification *probabilities* from the model rather than just class predictions. 

```{r}
prob_predictions <- predict(glm_fit, new_data = test_data, type = "prob") 
prob_results <- bind_cols(test_data, prob_predictions) %>% 
  dplyr::select(ER_Status,ESR1,last_col(),last_col(1)) %>% 
  pivot_longer(contains(".pred"), values_to = "Prob",names_to = "Class")

ggplot(prob_results, aes(x = ESR1, y = Prob,col=Class)) + geom_point()

```

## Decision Tree example


```{r}
# Specify the model
dt_spec <- decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("classification")

# Fit the model
dt_fit <- fit(dt_spec, ER_Status ~ ESR1, data = train_data)
# Print the model summary
dt_fit
```
```{r}
library(rpart.plot)

# Visualize the tree
rpart.plot(dt_fit$fit)
```


```{r}
ggplot(train_data, aes(x = ER_Status, y = ESR1)) + geom_boxplot() + geom_hline(yintercept = 12)
```

```{r}
library(yardstick)

predictions <- predict(dt_fit, new_data = test_data,type="class")
metrics <- augment(dt_fit, new_data = test_data)



# Calculate metrics
Metric_Summary <- bind_rows(Metric_Summary, data.frame(Accuracy = accuracy(metrics, truth = ER_Status, estimate = .pred_class)$.estimate, 
                  Precision=precision(metrics, truth = ER_Status, estimate = .pred_class)$.estimate,
                  Recall = recall(metrics, truth = ER_Status, estimate = .pred_class)$.estimate,
                  F1 = f_meas(metrics, truth = ER_Status, estimate = .pred_class)$.estimate,
                  Method = "Decision Tree")
)

Metric_Summary
```
## Adding more predictors

```{r}
train_data <- dplyr::select(train_data, ACTR3B:UBE2T,ER_Status)
test_data <- dplyr::select(test_data, ACTR3B:UBE2T,ER_Status)

# Fit the model
dt_fit <- fit(dt_spec, ER_Status ~ ., data = train_data)
# Print the model summary
dt_fit
```

```{r}
# Visualize the tree
rpart.plot(dt_fit$fit)
```


```{r}
predictions <- predict(dt_fit, new_data = test_data,type="class")
metrics <- augment(dt_fit, new_data = test_data)

# Calculate metrics
Metric_Summary <- bind_rows(Metric_Summary, data.frame(Accuracy = accuracy(metrics, truth = ER_Status, estimate = .pred_class)$.estimate, 
                  Precision=precision(metrics, truth = ER_Status, estimate = .pred_class)$.estimate,
                  Recall = recall(metrics, truth = ER_Status, estimate = .pred_class)$.estimate,
                  F1 = f_meas(metrics, truth = ER_Status, estimate = .pred_class)$.estimate,
                  Method = "Decision Tree 2")
)
Metric_Summary
```

```{r}
library(vip)
vip(dt_fit)
```

## Trying a random forest

```{r}
# Specify the random forest model

my_rec <- recipe(ER_Status ~., data = train_data)

my_prep <- prep(my_rec)
juiced <- juice(my_prep)

tune_spec <- rand_forest(
  mtry = tune(),
  trees = 1000,
  min_n = tune()
) %>%
  set_mode("classification") %>%
  set_engine("ranger")

tune_wf <- workflow() %>%
  add_recipe(my_rec) %>%
  add_model(tune_spec)

```


```{r}
set.seed(234)
train_folds <- vfold_cv(train_data)

doParallel::registerDoParallel()

set.seed(345)
tune_res <- tune_grid(
  tune_wf,
  resamples = train_folds,
  grid = 20
)

tune_res

```
```{r}
tune_res %>%
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  dplyr::select(mean, min_n, mtry) %>%
  pivot_longer(min_n:mtry,
    values_to = "value",
    names_to = "parameter"
  ) %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "AUC")
```

```{r}
rf_grid <- grid_regular(
  mtry(range = c(10, 20)),
  min_n(range = c(2, 8)),
  levels = 5
)

rf_grid
```

```{r}
set.seed(456)
regular_res <- tune_grid(
  tune_wf,
  resamples = train_folds,
  grid = rf_grid
)

regular_res
```

```{r}
regular_res %>%
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  mutate(min_n = factor(min_n)) %>%
  ggplot(aes(mtry, mean, color = min_n)) +
  geom_line(alpha = 0.5, size = 1.5) +
  geom_point() +
  labs(y = "AUC")
```


```{r}
best_auc <- select_best(regular_res, metric = "roc_auc")

final_rf <- finalize_model(
  tune_spec,
  best_auc
)

final_rf
```

```{r}
library(vip)

final_rf %>%
  set_engine("ranger", importance = "permutation") %>%
  fit(ER_Status ~ .,
    data = juice(tree_prep)
  ) %>%
  vip(geom = "point")
```


```{r}
predictions <- predict(wf_fit, new_data = test_data)
metrics <- augment(wf_fit, new_data = test_data)

Metric_Summary <- bind_rows(Metric_Summary, data.frame(Accuracy = accuracy(metrics, truth = ER_Status, estimate = .pred_class)$.estimate, 
                  Precision=precision(metrics, truth = ER_Status, estimate = .pred_class)$.estimate,
                  Recall = recall(metrics, truth = ER_Status, estimate = .pred_class)$.estimate,
                  F1 = f_meas(metrics, truth = ER_Status, estimate = .pred_class)$.estimate,
                  Method = "Random Forest"))
                  
```


